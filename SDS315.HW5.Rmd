---
title: "SDS.315.HW4"
author: "Aggie Angeles"
date: "2024-02-22"
output: html_document
---

EID: ana3636
<https://github.com/aa-squared/SDS315.HW5/tree/main>

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)

# --- libraries

library(mosaic)
library(ggplot2)
library(tidyverse)
library(kableExtra)
library(dbplyr)

```

## *Problem 1*
### *Iron Bank*

Are the observed data (70 flagged trades out of 2021) consistent with the SEC’s null hypothesis?

Our null hypothesis is that the Securities and Exchange Commission (SEC) mistakenly flags 2.4 legal trades per 100 trades (2.4%), on average over the long run. The test statistic is the number of flagged trades from the Iron Bank. A higher number of tradings that were flagged are consistent with stronger evidence against the null hypothesis. Our data states that 70 trades were flagged out of 2021. 

Assuming that the null hypothesis is true, we calculate the probability distribution of the test statistic using a monte carlo simulation of 2021 flips of a metaphorically biased coin that has a probability of turning up heads 4.7% of the time.  

2021 represents the number of securities trades from the Iron Bank. 2.4% is the estimated baseline probability that legal trade will be flagged by the SEC algorithm. 

 
From the simulation, we calculate a p-value of .0024 or 24 flagged trades out of 10,000. Based off the p-value, we find the the chances of 70 flagged trades out of 2021 to occur purely by chance to be implausible. We reject the null hypothesis.



```{r echo = FALSE, mesage = FALSE, include=FALSE}

# --- the details
# 70 flagged trades out of 2021
# baseline probability flagged is 2.4%

simFlagTrade <- do(100000)*nflip(n=2021, prob=0.024) 
amountFlag <- sum(simFlagTrade >= 70) # 24
pvalAmountFlag <- amountFlag / 100000 #.00024
print(pvalAmountFlag)



```

```{r echo = FALSE}

# --- GRAPH

ggplot(simFlagTrade) + 
  geom_histogram(aes(x=nflip), binwidth = 1) + labs( title = "Probability Distribution: Iron Bank Flagged Trades", x = "Flagged Security Trades")

```

## *Problem 2*
### *Health Inspections*

Our null hypothesis is that the Health Department cite 3 health violations per 100 (3%) restaurants in the city, on average over the long run. The test statistic is the number of health code violations received by Gourmet Bites restaurants. A higher number of health code violations cited are consistent with stronger evidence against the null hypothesis. Our data states that 8 Gourmet Bites received citations of 50.  

Assuming that the null hypothesis is true, we calculate the probability distribution of the test statistic using a Monte Carlo simulation of 1500 flips of a metaphorically biased coin that has a probability of turning up heads 3% of the time.  

1500 represents the number of restaurants inspected by the Health Department. 3% is the estimated baseline probability that any restaurants in the city will receive a health code citation by the Health Department algorithm. 
 
From the simulation, we calculate a p-value of  0.00011 or 11 citations out of 100,000. Based off the p-value, we find the the chances of 8 citations out of 50 to occur purely by chance to be implausible. We reject the null hypothesis.

```{r echo = FALSE, mesage = FALSE, include=FALSE}

# --- Gourmet Bites v.s citywide typical

# Gourmet Bites:
probGoBite <- 8/50 

# Citywide typical:
probCitywide <- .03

# incident ratio:
inciRatio <- round(probGoBite / probCitywide, 4)


simHealthGour <- do(100000)*nflip(n=50, prob=0.03) 
print(amountHeal <- sum(simHealthGour >= 8)) # 11
print(pvalAmountHeal <- amountHeal / 100000) # .00011



```



```{r echo = FALSE}

# --- GRAPH

ggplot(simHealthGour) + 
  geom_histogram(aes(x=nflip), binwidth = 1) + labs( title = "Probability Distribution: Health Department Inspections", x = "Inspection Violations")

```





## *Problem 3*
## *LLM Watermarking: Chi-Squared Goodness of Fit Statistic*

What does the chi-squared statistic look like across lots of normal English sentences not generated by an LLM?

### *Part A: The null or reference distribution*


```{r echo = FALSE, mesage = FALSE, include=FALSE}
# --- Import text into environ

sentences <- readLines("brown_sentences.txt")
head(sentences)

brown_sentences <- tibble(Sentences = sentences)
  
# Import letter frequencies
letter_frequencies = read.csv("letter_frequencies.csv")

```



```{r echo = FALSE, mesage = FALSE, include=FALSE}


calculate_obs_exp = function(sentence, freq_table) {
  
  # Ensure letter frequencies are normalized and sum to 1
  freq_table$Probability = freq_table$Probability / sum(freq_table$Probability)
  
  # Remove non-letters and convert to uppercase
  clean_sentence = gsub("[^A-Za-z]", "", sentence)
  clean_sentence = toupper(clean_sentence)
  
  # Count the occurrences of each letter in the sentence
  observed_counts = table(factor(strsplit(clean_sentence, "")[[1]], levels = freq_table$Letter))
  
  # Calculate expected counts
  total_letters = sum(observed_counts)
  expected_counts = total_letters * freq_table$Probability
  

  return(list("observed_counts" = observed_counts, "expected_counts" = expected_counts))
}


calculate_obs_exp(brown_sentences, letter_frequencies)



# --- function & inputs

null_distri = numeric(nrow(brown_sentences))

# num of observations 

for (i in 1:nrow(brown_sentences)) {
  
  residuals <- calculate_obs_exp(brown_sentences$Sentences[i], freq_table = letter_frequencies)
  
  chi_squared_stat = sum((residuals$observed_counts - residuals$expected_counts)^2 
                         / residuals$expected_counts)
  
  null_distri[i] <- chi_squared_stat
  
  
}
```

```{r echo = FALSE}
brown_sentences$ChiSquared <- null_distri

ggplot(brown_sentences) +
  geom_histogram(aes(x = ChiSquared)) + labs( title = "Chi-Squared Null Distribution of English Letters", x = "ChiSquared Values")

```


### *Part B: Checking for a watermark*


```{r echo = FALSE, mesage = FALSE, include=FALSE}


# Define the sentences
samp_sentences <- c(
  "She opened the book and started to read the first chapter, eagerly anticipating what might     come next.",
  "Despite the heavy rain, they decided to go for a long walk in the park, crossing the main     avenue by the fountain in the center.",
  "The museum’s new exhibit features ancient artifacts from various civilizations around the     world.",
  "He carefully examined the document, looking for any clues that might help solve the           mystery.",
  "The students gathered in the auditorium to listen to the guest speaker’s inspiring            lecture.",
  "Feeling vexed after an arduous and zany day at work, she hoped for a peaceful and quiet       evening at home, cozying up after a quick dinner with some TV, or maybe a book on her        upcoming visit to Auckland.",
  "The chef demonstrated how to prepare a delicious meal using only locally sourced              ingredients, focusing mainly on some excellent dinner recipes from Spain.",
  "They watched the sunset from the hilltop, marveling at the beautiful array of colors in       the sky.",
  "The committee reviewed the proposal and provided many points of useful feedback to improve     the project’s effectiveness.",
  "Despite the challenges faced during the project, the team worked tirelessly to ensure its     successful completion, resulting in a product that exceeded everyone’s expectations."
)

# Print the vector of sentences
print(samp_sentences)


```



```{r echo = FALSE, mesage = FALSE, include=FALSE}
# --- Find the p-value of each sentence

samp_sentences <- tibble(Sentences = samp_sentences)
sample_distri = numeric(nrow(samp_sentences))



# num of observations 

for (i in 1:nrow(samp_sentences)) {
  
  residuals <- calculate_obs_exp(samp_sentences$Sentences[i], freq_table = letter_frequencies)
  
  chi_squared_stat = sum((residuals$observed_counts - residuals$expected_counts)^2 
                         / residuals$expected_counts)
  
  sample_distri[i] <- chi_squared_stat
  
  
}



samp_sentences$ChiSquared <- sample_distri

 
samp_sentences$pval <- sapply(sample_distri, function(ChiSquare) {
  sum(null_distri >= ChiSquare) / length(null_distri)
})

samp_sentences

samp_sentences_pval <- samp_sentences %>%
  select(pval) 

samp_sentences_pval <- samp_sentences_pval %>%
  mutate(pval = round(pval, 3)) 

samp_sentences_pval <- samp_sentences_pval %>% 
  mutate(Sentence = c(1:10))


samp_sentences_pval <- samp_sentences_pval %>% 
  select(Sentence, pval)

sentenceKbl = kbl(samp_sentences_pval, col.names = c("Sentence", "P-Value"), caption = "English Letter Distribution",)

min(samp_sentences_pval$pval)

```

```{r echo = FALSE}

sentenceKbl %>%
  kable_classic(full_width = F, html_font = "Cambria", font_size = 12)

```

Sentence six: "Feeling vexed after an arduous and zany day at work, she hoped for a peaceful and quiet       evening at home, cozying up after a quick dinner with some TV, or maybe a book on her upcoming visit to Auckland," is the sentence produced by a large language model. When considering the the smaller the p-value the strong the evidence against the null hypothesis, in this our Chi-Square Distribution of English Letters, we find that the minimum p-value among the 10 sentences is sentence 6, with a English letter distribution p-value of .009. Additionally this p-value is three decimal places away where as the others have a decimal place in the tenths, or hundredths place. 
